{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiT import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"./\")\n",
    "\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Trains a new DiT model without using distributed training.\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n",
    "\n",
    "    if not os.path.exists(args.results_dir):\n",
    "        os.mkdir(args.results_dir)\n",
    "\n",
    "    # Create model:\n",
    "    # Note: You need to define DiT_models, create_diffusion, and other required objects\n",
    "    #       or import them from your implementation.\n",
    "    model = DiT_models[args.model](\n",
    "        input_size=args.image_size,\n",
    "        num_classes=args.num_classes,\n",
    "        class_dropout_prob=args.class_dropout_prob\n",
    "    )\n",
    "    ema = deepcopy(model).cuda()  # Create an EMA of the model for use after training\n",
    "    requires_grad(ema, False)\n",
    "    model = model.cuda()\n",
    "    diffusion = create_diffusion(timestep_respacing=\"\")\n",
    "    # vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").cuda()\n",
    "\n",
    "    # Setup optimizer\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=3000, gamma=0.8)\n",
    "    # Setup data:\n",
    "    data = torch.load(args.data_file)\n",
    "    dataset = TensorDataset(torch.tensor(data).cpu())\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.global_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    # Variables for monitoring/logging purposes:\n",
    "    train_steps = 0\n",
    "    log_steps = 0\n",
    "    running_loss = 0\n",
    "    start_time = time()\n",
    "\n",
    "    print(f\"Training for {args.epochs} epochs...\")\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Beginning epoch {epoch}...\")\n",
    "        for x in tqdm(loader):\n",
    "            x = x[0].to(torch.float32).cuda()\n",
    "            y = torch.zeros(x.shape[0], dtype=torch.int).cuda()\n",
    "            # print('input x shape: ',x.shape)\n",
    "            # with torch.no_grad():\n",
    "            #     x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "            #     print(x.shape)\n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],)).cuda()\n",
    "            model_kwargs = dict(y=y)\n",
    "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            update_ema(ema, model)\n",
    "\n",
    "            # Log loss values:\n",
    "            running_loss += loss.item()\n",
    "            log_steps += 1\n",
    "            train_steps += 1\n",
    "            if train_steps % args.log_every == 0:\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = time()\n",
    "                steps_per_sec = log_steps / (end_time - start_time)\n",
    "                avg_loss = torch.tensor(running_loss / log_steps).cuda()\n",
    "                logging.info(f'Epoch {epoch}, Step {train_steps}: Loss = {avg_loss}')\n",
    "                print(f\"(step={train_steps:07d}) Train Loss: {avg_loss:.4f}, Train Steps/Sec: {steps_per_sec:.2f}\")\n",
    "                running_loss = 0\n",
    "                log_steps = 0\n",
    "                start_time = time()\n",
    "                \n",
    "        # Save DiT checkpoint:\n",
    "        checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"ema\": ema.state_dict(),\n",
    "                \"opt\": opt.state_dict(),\n",
    "        }\n",
    "        checkpoint_path = f\"{args.results_dir}/ep{epoch}_{train_steps:07d}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_videos=10000\n",
    "seq_length=240\n",
    "repeat=0\n",
    "\n",
    "args_dict = {\n",
    "                    \"nnodes\":1,\n",
    "                    \"nproc_per_node\":\"N\",\n",
    "                    \"data_file\":\"latent_data_ball_10_240_bounce.pt\",\n",
    "                    \"results_dir\": \"./video_data/video_results_testQKV_real\",\n",
    "                    \"model\": f\"DiT-PD/2_N=240\",\n",
    "                    \"image_size\": 2,\n",
    "                    \"num_classes\": 1000,\n",
    "                    \"epochs\": 1000,\n",
    "                    \"global_batch_size\": 256,\n",
    "                    \"global_seed\": 0,\n",
    "                    \"vae\": \"ema\",\n",
    "                    \"num_workers\": 4,\n",
    "                    \"log_every\": 100,\n",
    "                    \"ckpt_every\": 50000,\n",
    "                    \"class_dropout_prob\": 1\n",
    "                }\n",
    "\n",
    "args = ArgsDict(args_dict)\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
