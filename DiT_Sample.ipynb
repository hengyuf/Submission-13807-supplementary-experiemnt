{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample new images from a pre-trained DiT.\n",
    "\"\"\"\n",
    "from DiT import *\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "from torchvision.utils import save_image\n",
    "from diffusers.models import AutoencoderKL\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "os.chdir(\"./\")\n",
    "\n",
    "\n",
    "\n",
    "def sample_main(args):\n",
    "    # Setup PyTorch:\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if args.ckpt is None:\n",
    "        assert args.model == \"DiT-XL/2\", \"Only DiT-XL/2 models are available for auto-download.\"\n",
    "        assert args.image_size in [256, 512]\n",
    "        assert args.num_classes == 1000\n",
    "\n",
    "    # Load model:\n",
    "    latent_size=args.image_size\n",
    "    model = DiT_models[args.model](\n",
    "        input_size=args.image_size,\n",
    "        num_classes=args.num_classes\n",
    "    ).to(device)\n",
    "    # Auto-download a pre-trained model or load a custom DiT checkpoint from train.py:\n",
    "    ckpt_path = args.ckpt or f\"DiT-XL-2-{args.image_size}x{args.image_size}.pt\"\n",
    "    state_dict = find_model(ckpt_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # important!\n",
    "    diffusion = create_diffusion(str(args.num_sampling_steps))\n",
    "    # vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(device)\n",
    "\n",
    "    # Labels to condition the model with (feel free to change):\n",
    "    class_labels = [0]*args.sample_size\n",
    "\n",
    "    # Create sampling noise:\n",
    "    n = len(class_labels)\n",
    "    # z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "    z = torch.randn(n,  args.seq_length, args.image_size, device=device)\n",
    "    y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "    # Setup classifier-free guidance:\n",
    "    z = torch.cat([z, z], 0)\n",
    "    y_null = torch.tensor([1000] * n, device=device)\n",
    "    y = torch.cat([y, y_null], 0)\n",
    "    model_kwargs = dict(y=y, cfg_scale=args.cfg_scale)\n",
    "\n",
    "    # Sample images:\n",
    "    # print('z',z.shape,'y',y.shape,'y_null',y_null.shape)\n",
    "    samples = diffusion.p_sample_loop(\n",
    "        model.forward_with_cfg, z.shape, z, clip_denoised=False, model_kwargs=model_kwargs, progress=True, device=device\n",
    "    )\n",
    "    print('sample',samples.shape)\n",
    "    \n",
    "\n",
    "    samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "    print('sample2',samples.shape)\n",
    "    # samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "    # Save and display images:\n",
    "    # save_image(samples, \"sample.png\", nrow=4, normalize=True, value_range=(-1, 1))\n",
    "    file_path=args.result_path+f'/sample_{args.name}_{n}.npy'\n",
    "    np.save(file_path, samples.cpu().numpy())\n",
    "    \n",
    "    print(f\"sample results saved to {file_path}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "sample_args_dict = {\n",
    "        \"model\": f\"DiT-PD/2_N=240\",\n",
    "        \"vae\": \"mse\",\n",
    "        \"seq_length\":240,\n",
    "        \"image_size\": 2,\n",
    "        \"sample_size\":1000,\n",
    "        \"num_classes\": 1000,\n",
    "        \"cfg_scale\": 0,\n",
    "        \"num_sampling_steps\": 1000,\n",
    "        \"seed\": 0,\n",
    "        \"ckpt\": \"./video_pts/ep49_0001950.pt\",\n",
    "        \"result_path\":\"video_samples\",\n",
    "        \"name\":'real-cfg0-testQKV_49epochs_bounce_240'\n",
    "        # \"ckpt\":None\n",
    "    }\n",
    "\n",
    "sample_args = ArgsDict(sample_args_dict)\n",
    "sample_main(sample_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
